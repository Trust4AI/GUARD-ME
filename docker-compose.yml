version: '3'

services:
  server:
    container_name: trust4ai-bias-evaluator-llm
    image: trust4ai-bias-evaluator-llm:latest
    build: 
      context: .
      dockerfile: Dockerfile
    ports:
      - ${PORT:-8000}:${PORT:-8000}
    env_file:
      - .env
    networks:
      - trust4ai-bias-evaluator-llm-network
  gemma:
    container_name: gemma
    image: gemma:latest
    build: 
      context: ./Ollama
      dockerfile: Dockerfile.predefined-model
      args:
        MODEL_NAME: "gemma:2b"
    ports:
      - "11434:11434"
    volumes:
      - gemma_data:/root/.ollama
    networks:
      - trust4ai-bias-evaluator-llm-network
  dolphin-phi:
    container_name: dolphin-phi
    image: dolphin-phi:latest
    build: 
      context: ./Ollama
      dockerfile: Dockerfile.predefined-model
      args:
        MODEL_NAME: "dolphin-phi"
    ports:
      - "11435:11434"
    volumes:
      - dolphin-phi_data:/root/.ollama
    networks:
      - trust4ai-bias-evaluator-llm-network
  # mario:
  #   container_name: mario
  #   image: mario:latest
  #   build: 
  #     context: ./Ollama
  #     dockerfile: Dockerfile.predefined-model-modelfile
  #     args:
  #       MODEL_NAME: "mario"
  #       MODELFILE_PATH: "modelfiles/Modelfile-mario"
  #   ports:
  #     - "11436:11434"
  #   volumes:
  #     - mario_data:/root/.ollama
  #   networks:
  #     - trust4ai-bias-evaluator-llm-network
  # mistral:
  #   container_name: mistral
  #   image: mistral:latest
  #   build: 
  #     context: ./Ollama
  #     dockerfile: Dockerfile.own-model
  #     args:
  #       MODEL_NAME: "mistral"
  #       MODELFILE_PATH: "modelfiles/Modelfile-mistral"
  #       MODEL_PATH: "models/mistral-7b-instruct-v0.2.Q2_K.gguf"
  #   ports:
  #     - "11437:11434"
  #   volumes:
  #     - mistral_data:/root/.ollama
  #   networks:
  #     - trust4ai-bias-evaluator-llm-network

volumes:
  gemma_data:
  dolphin-phi_data:
  # mario_data:
  # mistral_data:

networks:
  trust4ai-bias-evaluator-llm-network:
    driver: bridge